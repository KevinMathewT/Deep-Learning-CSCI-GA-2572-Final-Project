{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder2D(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 16, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Encoder2D(nn.Module):\n",
    "    def __init__(self, repr_dim, input_size=65):\n",
    "        super().__init__()\n",
    "        self.repr_dim = repr_dim\n",
    "        self.output_side = int(math.sqrt(repr_dim))  # Calculate the side of the 2D embedding\n",
    "\n",
    "        # Determine the number of convolutional blocks required\n",
    "        self.num_conv_blocks = int(math.log2(input_size / self.output_side))\n",
    "        if 2 ** self.num_conv_blocks * self.output_side != 2 ** int(math.log2(input_size)):\n",
    "            raise ValueError(\"Cannot evenly reduce input_size to output_side using stride-2 convolutions.\")\n",
    "\n",
    "        layers = []\n",
    "        in_channels = 2  # Input has 2 channels (agent and wall)\n",
    "        out_channels = 32  # Start with 32 output channels\n",
    "        for i in range(self.num_conv_blocks):\n",
    "            layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=0,\n",
    "                ) if i == 0 else\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                )\n",
    "            )  # Halve the spatial dimensions\n",
    "            layers.append(nn.ReLU())\n",
    "            in_channels = out_channels\n",
    "            out_channels = min(out_channels * 2, 256)  # Cap channels at 256\n",
    "\n",
    "        # Final convolution to reduce to single-channel output\n",
    "        layers.append(nn.Conv2d(in_channels, 1, kernel_size=1))  # Single-channel embedding\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (B, 2, 65, 65)\n",
    "        x = self.conv(x)  # Dynamically reduce to (B, 1, output_side, output_side)\n",
    "        return x  # Output shape: (B, 1, output_side, output_side)\n",
    "\n",
    "# Instantiate the Encoder2D with input size 65x65 and repr_dim 256\n",
    "encoder = Encoder2D(repr_dim=256, input_size=65)\n",
    "print(encoder)\n",
    "\n",
    "# Test with a dummy input\n",
    "input_tensor = torch.randn(1, 2, 65, 65)  # Batch size of 1, 2 channels, 65x65 input\n",
    "output = encoder(input_tensor)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 17, 17])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class FlexibleEncoder2D(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.repr_dim = config.embed_dim\n",
    "\n",
    "        # Ensure output size is consistent\n",
    "        self.output_side = int(math.sqrt(self.repr_dim))  # Should be 16 for config.embed_dim=256\n",
    "        if self.output_side != 16:\n",
    "            raise ValueError(\"Output side must be 16 for config.embed_dim=256.\")\n",
    "\n",
    "        # Dynamically select backbone based on config.encoder_backbone\n",
    "        self.backbone = timm.create_model(\n",
    "            config.encoder_backbone,  # Example: 'resnet18.a1_in1k'\n",
    "            pretrained=False,  # No pretraining allowed\n",
    "            num_classes=0,  # No classifier head\n",
    "            in_chans=2,  # Input has 2 channels\n",
    "            features_only=True,  # Extract spatial features\n",
    "        )\n",
    "\n",
    "        # Inspect available feature maps\n",
    "        self.feature_channels = [info['num_chs'] for info in self.backbone.feature_info]\n",
    "        self.feature_shapes = [info['reduction'] for info in self.backbone.feature_info]  # Spatial size reductions\n",
    "\n",
    "        # Select the layer closest to 16x16\n",
    "        self.closest_layer_index = self._find_closest_layer()\n",
    "\n",
    "        # Final adjustment to 16x16\n",
    "        self.adjust_to_target = nn.Sequential(\n",
    "            nn.Conv2d(self.feature_channels[self.closest_layer_index], 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def _find_closest_layer(self):\n",
    "        # Find the layer whose spatial size is closest to output_side\n",
    "        input_size = 65  # Assumes input spatial dimensions (H, W) = 65x65\n",
    "        reductions = [input_size // red for red in self.feature_shapes]\n",
    "        closest_index = min(range(len(reductions)), key=lambda i: abs(reductions[i] - self.output_side))\n",
    "        return closest_index\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the backbone and select the appropriate layer\n",
    "        features = self.backbone(x)\n",
    "        x = features[self.closest_layer_index]  # Closest layer to 16x16\n",
    "\n",
    "        # Adjust to target shape\n",
    "        x = self.adjust_to_target(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the configuration class\n",
    "class Config:\n",
    "    embed_dim = 256  # Output embedding size\n",
    "    encoder_backbone = 'resnet18.a1_in1k'  # Use ResNet-18 as the backbone\n",
    "\n",
    "# Instantiate and test the model\n",
    "config = Config()\n",
    "model = FlexibleEncoder2D(config)\n",
    "\n",
    "# Generate a random input tensor\n",
    "input_tensor = torch.randn(4, 2, 65, 65)  # Example input (B, 2, 65, 65)\n",
    "\n",
    "# Run the model and check the output size\n",
    "output = model(input_tensor)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 64, 17, 17])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convnext = timm.create_model('resnet18.a1_in1k', pretrained=False, num_classes=0, in_chans=2, features_only=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input to merge batch and trajectory dimensions\n",
    "        original_shape = x.shape\n",
    "        x = x.view(-1, *original_shape[-3:])  # Reshape to [batch*trajectory, channels, height, width]\n",
    "        features = self.convnext(x)[1]\n",
    "        \n",
    "        # Reshape features back to original trajectory structure\n",
    "        features = features.view(original_shape[0], original_shape[1], *features.shape[-3:])\n",
    "        return features\n",
    "\n",
    "\n",
    "# Define the configuration class\n",
    "class Config:\n",
    "    embed_dim = 256  # Output embedding size\n",
    "    encoder_backbone = 'resnet18.a1_in1k'  # Use ResNet-18 as the backbone\n",
    "\n",
    "# Instantiate and test the model\n",
    "config = Config()\n",
    "model = Encoder()\n",
    "\n",
    "# Generate a random input tensor\n",
    "input_tensor = torch.randn(4, 4, 2, 65, 65)  # Example input (B, 2, 65, 65)\n",
    "\n",
    "# Run the model and check the output size\n",
    "output = model(input_tensor)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 64, 17, 17])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, input_dim-2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(input_dim-2, output_dim, kernel_size=3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, encoded_o_t, action):\n",
    "        # Reshape inputs\n",
    "        batch_size, trajectory_length = encoded_o_t.shape[:2]\n",
    "        \n",
    "        # Reshape action to match encoded_o_t dimensions\n",
    "        action = action.view(batch_size, trajectory_length-1, 2, 1, 1)\n",
    "        action = action.repeat(1, 1, 1, encoded_o_t.size(3), encoded_o_t.size(4))\n",
    "        \n",
    "        # Prepare inputs for prediction\n",
    "        predictions = []\n",
    "        for t in range(trajectory_length - 1):\n",
    "            # Concatenate current encoded state with action\n",
    "            x = torch.cat([encoded_o_t[:, t], action[:, t]], dim=1)\n",
    "            pred = self.predictor(x)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        return torch.stack(predictions, dim=1)\n",
    "    \n",
    "# Generate a random action tensor\n",
    "action_tensor = torch.randn(4, 3, 2)  # Example action (B, T-1, 2)\n",
    "\n",
    "# Pass the encoded output and action through the predictor\n",
    "predictor = Predictor(input_dim=66, output_dim=64)  # Assuming input_dim=3 (encoded_o_t channels + action channels) and output_dim=1\n",
    "predicted_output = predictor(output, action_tensor)\n",
    "\n",
    "# Check the output size\n",
    "predicted_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
