{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder2D(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 16, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Encoder2D(nn.Module):\n",
    "    def __init__(self, repr_dim, input_size=65):\n",
    "        super().__init__()\n",
    "        self.repr_dim = repr_dim\n",
    "        self.output_side = int(math.sqrt(repr_dim))  # Calculate the side of the 2D embedding\n",
    "\n",
    "        # Determine the number of convolutional blocks required\n",
    "        self.num_conv_blocks = int(math.log2(input_size / self.output_side))\n",
    "        if 2 ** self.num_conv_blocks * self.output_side != 2 ** int(math.log2(input_size)):\n",
    "            raise ValueError(\"Cannot evenly reduce input_size to output_side using stride-2 convolutions.\")\n",
    "\n",
    "        layers = []\n",
    "        in_channels = 2  # Input has 2 channels (agent and wall)\n",
    "        out_channels = 32  # Start with 32 output channels\n",
    "        for i in range(self.num_conv_blocks):\n",
    "            layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=0,\n",
    "                ) if i == 0 else\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                )\n",
    "            )  # Halve the spatial dimensions\n",
    "            layers.append(nn.ReLU())\n",
    "            in_channels = out_channels\n",
    "            out_channels = min(out_channels * 2, 256)  # Cap channels at 256\n",
    "\n",
    "        # Final convolution to reduce to single-channel output\n",
    "        layers.append(nn.Conv2d(in_channels, 1, kernel_size=1))  # Single-channel embedding\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (B, 2, 65, 65)\n",
    "        x = self.conv(x)  # Dynamically reduce to (B, 1, output_side, output_side)\n",
    "        return x  # Output shape: (B, 1, output_side, output_side)\n",
    "\n",
    "# Instantiate the Encoder2D with input size 65x65 and repr_dim 256\n",
    "encoder = Encoder2D(repr_dim=256, input_size=65)\n",
    "print(encoder)\n",
    "\n",
    "# Test with a dummy input\n",
    "input_tensor = torch.randn(1, 2, 65, 65)  # Batch size of 1, 2 channels, 65x65 input\n",
    "output = encoder(input_tensor)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 17, 17])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class FlexibleEncoder2D(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.repr_dim = config.embed_dim\n",
    "\n",
    "        # Ensure output size is consistent\n",
    "        self.output_side = int(math.sqrt(self.repr_dim))  # Should be 16 for config.embed_dim=256\n",
    "        if self.output_side != 16:\n",
    "            raise ValueError(\"Output side must be 16 for config.embed_dim=256.\")\n",
    "\n",
    "        # Dynamically select backbone based on config.encoder_backbone\n",
    "        self.backbone = timm.create_model(\n",
    "            config.encoder_backbone,  # Example: 'resnet18.a1_in1k'\n",
    "            pretrained=False,  # No pretraining allowed\n",
    "            num_classes=0,  # No classifier head\n",
    "            in_chans=2,  # Input has 2 channels\n",
    "            features_only=True,  # Extract spatial features\n",
    "        )\n",
    "\n",
    "        # Inspect available feature maps\n",
    "        self.feature_channels = [info['num_chs'] for info in self.backbone.feature_info]\n",
    "        self.feature_shapes = [info['reduction'] for info in self.backbone.feature_info]  # Spatial size reductions\n",
    "\n",
    "        # Select the layer closest to 16x16\n",
    "        self.closest_layer_index = self._find_closest_layer()\n",
    "\n",
    "        # Final adjustment to 16x16\n",
    "        self.adjust_to_target = nn.Sequential(\n",
    "            nn.Conv2d(self.feature_channels[self.closest_layer_index], 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def _find_closest_layer(self):\n",
    "        # Find the layer whose spatial size is closest to output_side\n",
    "        input_size = 65  # Assumes input spatial dimensions (H, W) = 65x65\n",
    "        reductions = [input_size // red for red in self.feature_shapes]\n",
    "        closest_index = min(range(len(reductions)), key=lambda i: abs(reductions[i] - self.output_side))\n",
    "        return closest_index\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the backbone and select the appropriate layer\n",
    "        features = self.backbone(x)\n",
    "        x = features[self.closest_layer_index]  # Closest layer to 16x16\n",
    "\n",
    "        # Adjust to target shape\n",
    "        x = self.adjust_to_target(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the configuration class\n",
    "class Config:\n",
    "    embed_dim = 256  # Output embedding size\n",
    "    encoder_backbone = 'resnet18.a1_in1k'  # Use ResNet-18 as the backbone\n",
    "\n",
    "# Instantiate and test the model\n",
    "config = Config()\n",
    "model = FlexibleEncoder2D(config)\n",
    "\n",
    "# Generate a random input tensor\n",
    "input_tensor = torch.randn(4, 2, 65, 65)  # Example input (B, 2, 65, 65)\n",
    "\n",
    "# Run the model and check the output size\n",
    "output = model(input_tensor)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def print_param_count(model: nn.Module):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 11173376\n",
      "Trainable parameters: 11173376\n",
      "Total parameters: 0\n",
      "Trainable parameters: 0\n",
      "torch.Size([4, 4, 64, 33, 33]) torch.Size([4, 4, 2, 65, 65])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convnext = timm.create_model(\n",
    "            'resnet18.a1_in1k', \n",
    "            pretrained=False, \n",
    "            num_classes=0, \n",
    "            in_chans=2, \n",
    "            features_only=True\n",
    "        )\n",
    "        feature_info = self.convnext.feature_info\n",
    "        self.convnext2 = timm.create_model(\n",
    "            'resnet18.a1_in1k', \n",
    "            pretrained=False, \n",
    "            num_classes=0, \n",
    "            in_chans=2, \n",
    "            features_only=True, \n",
    "            out_indices=(1,)  # Only keep up to layer1\n",
    "        )\n",
    "        print_param_count(self.convnext)\n",
    "        print_param_count(self.convnext2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input to merge batch and trajectory dimensions\n",
    "        original_shape = x.shape\n",
    "        x = x.view(-1, *original_shape[-3:])  # Reshape to [batch*trajectory, channels, height, width]\n",
    "        features = self.convnext(x)[1]\n",
    "        \n",
    "        # Reshape features back to original trajectory structure\n",
    "        features1 = features.view(original_shape[0], original_shape[1], *features.shape[-3:])\n",
    "\n",
    "        features = self.convnext2(x)[0]\n",
    "        \n",
    "        # Reshape features back to original trajectory structure\n",
    "        features2 = features.view(original_shape[0], original_shape[1], *features.shape[-3:])\n",
    "        return features1, features2\n",
    "\n",
    "\n",
    "# Define the configuration class\n",
    "class Config:\n",
    "    embed_dim = 256  # Output embedding size\n",
    "    encoder_backbone = 'resnet18.a1_in1k'  # Use ResNet-18 as the backbone\n",
    "\n",
    "# Instantiate and test the model\n",
    "config = Config()\n",
    "model = Encoder()\n",
    "\n",
    "# Generate a random input tensor\n",
    "input_tensor = torch.ones(4, 4, 2, 65, 65)  # Example input (B, 2, 65, 65)\n",
    "\n",
    "# Run the model and check the output size\n",
    "features1, features2 = model(input_tensor)\n",
    "print(features1.shape, features2.shape)\n",
    "\n",
    "print(features1[0][0][0][0])\n",
    "print(features2[0][0][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model parameters: 11.69 million\n",
      "Truncated model parameters: 0.15 million\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "# Function to calculate the number of parameters in a PyTorch model (in millions)\n",
    "def calculate_model_parameters(model):\n",
    "    \"\"\"\n",
    "    Calculate the number of parameters in a PyTorch model in millions.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): PyTorch model to calculate the parameters for.\n",
    "\n",
    "    Returns:\n",
    "        float: Number of parameters in millions.\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params / 1e6  # Convert to millions\n",
    "\n",
    "# Full ResNet-18 model\n",
    "full_model = timm.create_model('resnet18.a1_in1k', pretrained=False, in_chans=2)\n",
    "\n",
    "# Truncated model with features_only=True\n",
    "truncated_model = timm.create_model(\n",
    "    'resnet18.a1_in1k', \n",
    "    pretrained=False, \n",
    "    in_chans=2, \n",
    "    features_only=True, \n",
    "    out_indices=(1,)  # Only keep up to layer1\n",
    ")\n",
    "\n",
    "# Calculate and print number of parameters\n",
    "full_model_params = calculate_model_parameters(full_model)\n",
    "truncated_model_params = calculate_model_parameters(truncated_model)\n",
    "\n",
    "print(f\"Full model parameters: {full_model_params:.2f} million\")\n",
    "print(f\"Truncated model parameters: {truncated_model_params:.2f} million\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 64, 17, 17])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, input_dim-2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(input_dim-2, output_dim, kernel_size=3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, encoded_o_t, action):\n",
    "        # Reshape inputs\n",
    "        batch_size, trajectory_length = encoded_o_t.shape[:2]\n",
    "        \n",
    "        # Reshape action to match encoded_o_t dimensions\n",
    "        action = action.view(batch_size, trajectory_length-1, 2, 1, 1)\n",
    "        action = action.repeat(1, 1, 1, encoded_o_t.size(3), encoded_o_t.size(4))\n",
    "        \n",
    "        # Prepare inputs for prediction\n",
    "        predictions = []\n",
    "        for t in range(trajectory_length - 1):\n",
    "            # Concatenate current encoded state with action\n",
    "            x = torch.cat([encoded_o_t[:, t], action[:, t]], dim=1)\n",
    "            pred = self.predictor(x)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        return torch.stack(predictions, dim=1)\n",
    "    \n",
    "# Generate a random action tensor\n",
    "action_tensor = torch.randn(4, 3, 2)  # Example action (B, T-1, 2)\n",
    "\n",
    "# Pass the encoded output and action through the predictor\n",
    "predictor = Predictor(input_dim=66, output_dim=64)  # Assuming input_dim=3 (encoded_o_t channels + action channels) and output_dim=1\n",
    "predicted_output = predictor(output, action_tensor)\n",
    "\n",
    "# Check the output size\n",
    "predicted_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnext = timm.create_model('resnet18.a1_in1k', pretrained=False, num_classes=0, in_chans=2, features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureListNet(\n",
       "  (conv1): Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1170 [00:12<2:02:20,  6.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m tqdm(model_names):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# Create the model without a classifier head\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# Get the number of parameters\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         num_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n",
      "File \u001b[0;32m~/Desktop/Kevin/projects/Deep-Learning-CSCI-GA-2572-Final-Project/.venv/lib/python3.11/site-packages/timm/models/_factory.py:117\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n\u001b[0;32m--> 117\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path:\n\u001b[1;32m    125\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[0;32m~/Desktop/Kevin/projects/Deep-Learning-CSCI-GA-2572-Final-Project/.venv/lib/python3.11/site-packages/timm/models/beit.py:670\u001b[0m, in \u001b[0;36mbeit_base_patch16_384\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;129m@register_model\u001b[39m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbeit_base_patch16_384\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Beit:\n\u001b[1;32m    667\u001b[0m     model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    668\u001b[0m         img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m384\u001b[39m, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[1;32m    669\u001b[0m         use_abs_pos_emb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_rel_pos_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, init_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m--> 670\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_create_beit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeit_base_patch16_384\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Desktop/Kevin/projects/Deep-Learning-CSCI-GA-2572-Final-Project/.venv/lib/python3.11/site-packages/timm/models/beit.py:647\u001b[0m, in \u001b[0;36m_create_beit\u001b[0;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_beit\u001b[39m(variant, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    646\u001b[0m     out_indices \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_indices\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m--> 647\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model_with_cfg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBeit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_filter_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_filter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgetter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Desktop/Kevin/projects/Deep-Learning-CSCI-GA-2572-Final-Project/.venv/lib/python3.11/site-packages/timm/models/_builder.py:415\u001b[0m, in \u001b[0;36mbuild_model_with_cfg\u001b[0;34m(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, kwargs_filter, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_cls(cfg\u001b[38;5;241m=\u001b[39mmodel_cfg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Kevin/projects/Deep-Learning-CSCI-GA-2572-Final-Project/.venv/lib/python3.11/site-packages/timm/models/beit.py:355\u001b[0m, in \u001b[0;36mBeit.__init__\u001b[0;34m(self, img_size, patch_size, in_chans, num_classes, global_pool, embed_dim, depth, num_heads, qkv_bias, mlp_ratio, swiglu_mlp, scale_mlp, drop_rate, pos_drop_rate, proj_drop_rate, attn_drop_rate, drop_path_rate, norm_layer, init_values, use_abs_pos_emb, use_rel_pos_bias, use_shared_rel_pos_bias, head_init_scale)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_drop \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(drop_rate)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, num_classes) \u001b[38;5;28;01mif\u001b[39;00m num_classes \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     trunc_normal_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.02\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Kevin/projects/Deep-Learning-CSCI-GA-2572-Final-Project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1029\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 1029\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Kevin/projects/Deep-Learning-CSCI-GA-2572-Final-Project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1029\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 1029\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: Module.apply at line 1029 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Kevin/projects/Deep-Learning-CSCI-GA-2572-Final-Project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1029\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 1029\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Kevin/projects/Deep-Learning-CSCI-GA-2572-Final-Project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1030\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m   1029\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m-> 1030\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Kevin/projects/Deep-Learning-CSCI-GA-2572-Final-Project/.venv/lib/python3.11/site-packages/timm/models/beit.py:376\u001b[0m, in \u001b[0;36mBeit._init_weights\u001b[0;34m(self, m)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, m):\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[0;32m--> 376\u001b[0m         \u001b[43mtrunc_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.02\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, nn\u001b[38;5;241m.\u001b[39mLinear) \u001b[38;5;129;01mand\u001b[39;00m m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m             nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mconstant_(m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Kevin/projects/Deep-Learning-CSCI-GA-2572-Final-Project/.venv/lib/python3.11/site-packages/timm/layers/weight_init.py:67\u001b[0m, in \u001b[0;36mtrunc_normal_\u001b[0;34m(tensor, mean, std, a, b)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Fills the input Tensor with values drawn from a truncated\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03mnormal distribution. The values are effectively drawn from the\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03mnormal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    >>> nn.init.trunc_normal_(w)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_trunc_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Kevin/projects/Deep-Learning-CSCI-GA-2572-Final-Project/.venv/lib/python3.11/site-packages/timm/layers/weight_init.py:28\u001b[0m, in \u001b[0;36m_trunc_normal_\u001b[0;34m(tensor, mean, std, a, b)\u001b[0m\n\u001b[1;32m     24\u001b[0m u \u001b[38;5;241m=\u001b[39m norm_cdf((b \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Uniformly fill tensor with values from [l, u], then translate to\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# [2l-1, 2u-1].\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Use inverse cdf transform for normal distribution to get truncated\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# standard normal\u001b[39;00m\n\u001b[1;32m     32\u001b[0m tensor\u001b[38;5;241m.\u001b[39merfinv_()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize an empty list to store model details\n",
    "model_details = []\n",
    "\n",
    "# Get all available models in timm\n",
    "model_names = timm.list_models()\n",
    "\n",
    "# Loop through each model\n",
    "for model_name in tqdm(model_names):\n",
    "    try:\n",
    "        # Create the model without a classifier head\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=0)\n",
    "        # Get the number of parameters\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        # Add to the list\n",
    "        model_details.append((model_name, num_params))\n",
    "    except Exception as e:\n",
    "        # Some models may fail to initialize; skip them\n",
    "        print(f\"Error with model {model_name}: {e}\")\n",
    "\n",
    "# Sort models by size\n",
    "sorted_models = sorted(model_details, key=lambda x: x[1])\n",
    "\n",
    "# Display the models and their sizes\n",
    "for name, size in sorted_models:\n",
    "    print(f\"{name}: {size} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet_b0.ra_in1k\n",
      "Total parameters: 3595100\n",
      "Trainable parameters: 3595100\n",
      "Total parameters: 18802\n",
      "Trainable parameters: 18802\n",
      "Full Model Output Size: torch.Size([4, 24, 17, 17])\n",
      "Minimal Model Output Size: torch.Size([4, 24, 17, 17])\n",
      "{'stage': 1, 'reduction': 2, 'module': 'blocks.0', 'num_chs': 16, 'index': 0}\n",
      "{'stage': 2, 'reduction': 4, 'module': 'blocks.1', 'num_chs': 24, 'index': 1}\n",
      "{'stage': 3, 'reduction': 8, 'module': 'blocks.2', 'num_chs': 40, 'index': 2}\n",
      "{'stage': 5, 'reduction': 16, 'module': 'blocks.4', 'num_chs': 112, 'index': 3}\n",
      "{'stage': 7, 'reduction': 32, 'module': 'blocks.6', 'num_chs': 320, 'index': 4}\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Initialize the full model\n",
    "model_name = 'efficientnet_b0.ra_in1k'\n",
    "print(model_name)\n",
    "full_model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=False,\n",
    "    num_classes=0,\n",
    "    in_chans=2,\n",
    "    features_only=True\n",
    ")\n",
    "\n",
    "# Feature information\n",
    "feature_info = full_model.feature_info\n",
    "selected_feature_layer = feature_info[1]['module']  # 'blocks.1'\n",
    "\n",
    "# Create the base model\n",
    "base_model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=False,\n",
    "    num_classes=0,\n",
    "    in_chans=2\n",
    ")\n",
    "\n",
    "# Extract required layers\n",
    "layers = []\n",
    "for name, module in base_model.named_children():\n",
    "    if name == 'blocks':\n",
    "        # Go inside 'blocks' and only add layers up to 'blocks.1'\n",
    "        sublayers = []\n",
    "        for i, subblock in enumerate(module.children()):\n",
    "            sublayers.append((str(i), subblock))\n",
    "            if f'blocks.{i}' == selected_feature_layer:\n",
    "                break\n",
    "        partial_blocks = nn.Sequential(OrderedDict(sublayers))\n",
    "        layers.append(('blocks', partial_blocks))\n",
    "        break\n",
    "    else:\n",
    "        layers.append((name, module))\n",
    "\n",
    "# Build the minimal model\n",
    "minimal_model = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "# Cleanup\n",
    "del base_model\n",
    "\n",
    "# Compare parameter counts\n",
    "print(print_param_count(full_model))\n",
    "print(print_param_count(minimal_model))\n",
    "\n",
    "# Input tensor\n",
    "input_tensor = torch.ones(4, 2, 65, 65)  # Example input (B, 2, 65, 65)\n",
    "\n",
    "# Pass through the full model\n",
    "full_model_output = full_model(input_tensor)[1]  # Output from selected feature map\n",
    "\n",
    "# Pass through the minimal model\n",
    "minimal_model_output = minimal_model(input_tensor)\n",
    "\n",
    "# Print output sizes\n",
    "print(\"Full Model Output Size:\", full_model_output.shape)\n",
    "print(\"Minimal Model Output Size:\", minimal_model_output.shape)\n",
    "\n",
    "# Print feature info for reference\n",
    "for i in range(5):\n",
    "    print(feature_info[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureListNet(\n",
       "  (conv1): Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
